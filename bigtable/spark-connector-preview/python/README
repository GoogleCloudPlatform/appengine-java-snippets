# Spark Bigtable Example Using Python

This example uses Python to write data to a Bigtable table and read it back.

## Running the example using Dataproc

To submit the JAR to Dataproc, you will need a Bigtable project and
instance ID, as well as a Bigtable table name, which will be the three required
arguments. By default, a new table is created by the application, but you can
provide an optional fourth arguemnt `false` for `createNewTable` (assuming
that you have already created a table with the column family `example_family`).

To run the JAR using dataproc, you can run the following command:

```
gcloud dataproc jobs submit pyspark \
--cluster=$SPARK_BIGTABLE_DATAPROC_CLUSTER \
--region=$SPARK_BIGTABLE_DATAPROC_REGION \
--jars=gs://spark-bigtable-preview/jars/spark-bigtable-0.0.1-preview2-SNAPSHOT.jar \
word_count.py \
-- \
--bigtableProjectId=$SPARK_BIGTABLE_PROJECT_ID \
--bigtableInstanceId=$SPARK_BIGTABLE_INSTANCE_ID \
--bigtableTableName=$SPARK_BIGTABLE_TABLE_NAME
```

## Expected output

The following text should be shown in the output of the Spark job.

```
Reading the DataFrame from Bigtable:
+-----+-----+
|count| word|
+-----+-----+
|    0|word0|
|    1|word1|
|    2|word2|
|    3|word3|
|    4|word4|
|    5|word5|
|    6|word6|
|    7|word7|
|    8|word8|
|    9|word9|
+-----+-----+
```


To verify that the data has been written to Bigtable, you can run the following
command (requires [cbt CLI](https://cloud.google.com/bigtable/docs/cbt-overview)):

```
cbt -project=$SPARK_BIGTABLE_PROJECT_ID -instance=$SPARK_BIGTABLE_INSTANCE_ID \
read $SPARK_BIGTABLE_TABLE_NAME
```

With this expected output:
```
----------------------------------------
word0
  example_family:countCol                      @ 2023/07/11-16:05:59.596000
    "\x00\x00\x00\x00"

----------------------------------------
word1
  example_family:countCol                      @ 2023/07/11-16:05:59.611000
    "\x00\x00\x00\x01"

----------------------------------------
.
.
.
```
